{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNRIiHjBO6zZmA8lfjPAMPX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deepakjongra/MachineLearning/blob/main/data_preprocessing_and_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Complaint Classification using NLP</h1>"
      ],
      "metadata": {
        "id": "NKc8ckDbBKcU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "vIO8pBve_tDp"
      },
      "outputs": [],
      "source": [
        "!pip install pandas numpy transformers torch indic-nlp-library googletrans==4.0.0-rc1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import pipeline\n",
        "from googletrans import Translator\n",
        "from indicnlp.normalize import indic_normalize"
      ],
      "metadata": {
        "id": "UPiRNspEAOXY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = {\n",
        "    \"complaint_id\": [1, 2, 3, 4],\n",
        "    \"text\": [\n",
        "        \"पाणी नाही आहे, मदत करा!\",  # Marathi\n",
        "        \"बिजली कट हो गई है\",       # Hindi\n",
        "        \"There is a pothole on the road\",  # English\n",
        "        \"Garbage is not collected since 3 days\"  # English\n",
        "    ],\n",
        "    \"language\": [\"mr\", \"hi\", \"en\", \"en\"]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "VoIAwbc7AioF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translator = Translator()\n",
        "\n",
        "def translate_to_marathi(text, lang):\n",
        "    if lang == \"mr\":  # Already in Marathi\n",
        "        return text\n",
        "    try:\n",
        "        translated = translator.translate(text, src=lang, dest=\"mr\")\n",
        "        return translated.text\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "df[\"translated_text\"] = df.apply(lambda x: translate_to_marathi(x[\"text\"], x[\"language\"]), axis=1)\n",
        "\n",
        "df[[\"text\", \"translated_text\"]]\n"
      ],
      "metadata": {
        "id": "vZUGPZVYAnYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalizer = indic_normalize.DevanagariNormalizer()\n",
        "\n",
        "def normalize_text(text):\n",
        "    return normalizer.normalize(text)\n",
        "\n",
        "df[\"normalized_text\"] = df[\"translated_text\"].apply(normalize_text)\n",
        "df[[\"translated_text\", \"normalized_text\"]]\n"
      ],
      "metadata": {
        "id": "xcJgSutXAquf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch"
      ],
      "metadata": {
        "id": "mxvZlsvKA6yV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "EMsYJffLBGLn"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "# Define the categories from Maharashtra Govt Departments\n",
        "categories = [\n",
        "    \"Home Department\", \"Urban Development Department\", \"Public Works Department\",\n",
        "    \"Water Resources Department\", \"Energy Department\", \"Health Department\",\n",
        "    \"Education Department\", \"Food, Civil Supplies, and Consumer Protection Department\",\n",
        "    \"Environment and Climate Change Department\", \"Social Justice Department\",\n",
        "    \"Tribal Development Department\", \"Agriculture Department\", \"Labour Department\",\n",
        "    \"Transport Department\", \"Tourism Department\"\n",
        "]\n",
        "\n",
        "# Load IndicBERT Model and Tokenizer\n",
        "model_name = \"ai4bharat/indic-bert\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(categories))\n"
      ],
      "metadata": {
        "id": "-XjD6f3dBekw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_complaint_indicbert(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    predicted_class = torch.argmax(outputs.logits).item()\n",
        "    return categories[predicted_class]\n",
        "\n",
        "# Sample Complaint\n",
        "complaint_text = \"बिजली नहीं है, बहुत दिक्कत हो रही है।\"\n",
        "predicted_department = classify_complaint_indicbert(complaint_text)\n",
        "print(f\"Complaint: {complaint_text}\\nPredicted Department: {predicted_department}\")\n",
        "\n",
        "# Sample Complaints DataFrame\n",
        "data = {\n",
        "    \"complaint_text\": [\n",
        "        \"पानी की सप्लाई बंद है, कुछ कीजिए!\",\n",
        "        \"रास्ते में बहुत गड्ढे हैं, सफर मुश्किल हो गया है।\",\n",
        "        \"राशन की दुकान पर लूटखसोट हो रही है।\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Apply Classification to Each Complaint\n",
        "df[\"department\"] = df[\"complaint_text\"].apply(classify_complaint_indicbert)\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfuvM5VdDFq6",
        "outputId": "188fda29-c74a-4bc1-c883-7cdee63e858d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Complaint: बिजली नहीं है, बहुत दिक्कत हो रही है।\n",
            "Predicted Department: Environment and Climate Change Department\n",
            "                                      complaint_text  \\\n",
            "0                  पानी की सप्लाई बंद है, कुछ कीजिए!   \n",
            "1  रास्ते में बहुत गड्ढे हैं, सफर मुश्किल हो गया है।   \n",
            "2                राशन की दुकान पर लूटखसोट हो रही है।   \n",
            "\n",
            "                                  department  \n",
            "0  Environment and Climate Change Department  \n",
            "1  Environment and Climate Change Department  \n",
            "2  Environment and Climate Change Department  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Fine Tunning Model</h1>"
      ],
      "metadata": {
        "id": "JLaHN2o2Dkm9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch datasets sentencepiece scikit-learn"
      ],
      "metadata": {
        "id": "rMVsNORkDkW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Load Pretrained IndicBERT</h1>"
      ],
      "metadata": {
        "id": "O87Y6BLoDtw_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
        "from datasets import Dataset\n",
        "import torch\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define categories (Departments of Maharashtra Government) - Remove duplicate \"Labour Department\"\n",
        "categories = [\n",
        "    \"School Education and Sports\", \"Agriculture Department\", \"Co-operation, Marketing and Textiles Department\",\n",
        "    \"Dairy Development Department\", \"Department Of Animal Husbandry\", \"Department Of Minority Development\",\n",
        "    \"Department of Industries, Energy, Labor and Mines\", \"Department of Sainik Welfare\",\n",
        "    \"Environment and Climate Change Department\", \"Finance Department\",\n",
        "    \"Fisheries Department\", \"Food, Civil Supplies and Consumer Protection Department\", \"Forest Department\",\n",
        "    \"General Administration Department\", \"Law and Judiciary Department\", \"Higher and Technical Education Department\",\n",
        "    \"Housing Department\", \"Labour Department\", \"Greater Mumbai Police\"\n",
        "]\n",
        "\n",
        "# Load IndicBERT Model and Tokenizer\n",
        "model_name = \"ai4bharat/indic-bert\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(categories))"
      ],
      "metadata": {
        "id": "jqCOg0P1Dr-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"./complaints.csv\")\n",
        "\n",
        "# 🔹 Check for NaN values in the department column\n",
        "df = df.dropna(subset=[\"department\", \"complaint_text\"])  # Remove missing values\n",
        "\n",
        "# 🔹 Convert department names to numerical labels\n",
        "label_map = {dept: i for i, dept in enumerate(categories)}\n",
        "df[\"label\"] = df[\"department\"].map(label_map)\n",
        "\n",
        "# 🔹 Remove rows where the department is not in label_map (i.e., NaN labels)\n",
        "df = df.dropna(subset=[\"label\"])\n",
        "df[\"label\"] = df[\"label\"].astype(int)  # Ensure labels are integers\n",
        "\n",
        "# Split into Train & Validation\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    df[\"complaint_text\"].tolist(), df[\"label\"].tolist(), test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Tokenize Data\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512)\n",
        "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=512)\n",
        "\n",
        "# Convert to Hugging Face Dataset (Ensuring Labels are Torch LongTensors)\n",
        "train_dataset = Dataset.from_dict({\n",
        "    \"input_ids\": train_encodings[\"input_ids\"],\n",
        "    \"attention_mask\": train_encodings[\"attention_mask\"],\n",
        "    \"labels\": torch.tensor(train_labels, dtype=torch.long)  # 🔹 FIX: Ensure correct label format\n",
        "})\n",
        "\n",
        "val_dataset = Dataset.from_dict({\n",
        "    \"input_ids\": val_encodings[\"input_ids\"],\n",
        "    \"attention_mask\": val_encodings[\"attention_mask\"],\n",
        "    \"labels\": torch.tensor(val_labels, dtype=torch.long)  # 🔹 FIX: Ensure correct label format\n",
        "})\n",
        "\n"
      ],
      "metadata": {
        "id": "WPp5vg81D4eX"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=10,  # Increase training cycles\n",
        "    per_device_train_batch_size=16,  # If GPU can handle more\n",
        "    per_device_eval_batch_size=16,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=3e-5,  # Adjust learning rate\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1E95ry6Hmp9",
        "outputId": "30beb779-015b-4d42-b63f-c6c08810fb7e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Model\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "hqtXOlESHqoN",
        "outputId": "efd76e67-7fc2-40ed-e352-fc72ab3c85b4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [500/500 1:34:09, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.659791</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.145175</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.538072</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.154503</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.855028</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.635423</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.481551</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.383515</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.314073</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.174100</td>\n",
              "      <td>0.288564</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=500, training_loss=1.17413623046875, metrics={'train_runtime': 5664.2043, 'train_samples_per_second': 1.412, 'train_steps_per_second': 0.088, 'total_flos': 24686332704000.0, 'train_loss': 1.17413623046875, 'epoch': 10.0})"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define save path\n",
        "save_directory = \"./fine_tuned_indic_bert\"\n",
        "\n",
        "# Save model and tokenizer\n",
        "model.save_pretrained(save_directory)\n",
        "tokenizer.save_pretrained(save_directory)\n",
        "\n",
        "print(f\"Model saved to {save_directory}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79JNj83T2bF9",
        "outputId": "a8985609-f7de-464b-dae9-7d9d8bdaeaaf"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to ./fine_tuned_indic_bert\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = trainer.evaluate()\n",
        "print(metrics)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "hgrW6CR3YW5v",
        "outputId": "87b72fbe-30ef-454b-90f7-a67093a76952"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [13/13 00:40]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.2885638475418091, 'eval_runtime': 43.537, 'eval_samples_per_second': 4.594, 'eval_steps_per_second': 0.299, 'epoch': 10.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# Load the fine-tuned model\n",
        "model_name = \"./fine_tuned_indic_bert\"  # Path where your model is saved\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# Ensure model is on the correct device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "model.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKMK3bYLzmsA",
        "outputId": "eb108115-78b1-4e33-ba8c-bba683b8d684"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AlbertForSequenceClassification(\n",
              "  (albert): AlbertModel(\n",
              "    (embeddings): AlbertEmbeddings(\n",
              "      (word_embeddings): Embedding(200000, 128, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 128)\n",
              "      (token_type_embeddings): Embedding(2, 128)\n",
              "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0, inplace=False)\n",
              "    )\n",
              "    (encoder): AlbertTransformer(\n",
              "      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n",
              "      (albert_layer_groups): ModuleList(\n",
              "        (0): AlbertLayerGroup(\n",
              "          (albert_layers): ModuleList(\n",
              "            (0): AlbertLayer(\n",
              "              (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (attention): AlbertSdpaAttention(\n",
              "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (attention_dropout): Dropout(p=0, inplace=False)\n",
              "                (output_dropout): Dropout(p=0, inplace=False)\n",
              "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              )\n",
              "              (ffn): Linear(in_features=768, out_features=3072, bias=True)\n",
              "              (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n",
              "              (activation): GELUActivation()\n",
              "              (dropout): Dropout(p=0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (pooler_activation): Tanh()\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=19, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_texts = [\n",
        "    \"माझ्या गावातील रस्त्याची अवस्था खूपच वाईट आहे. पावसाळ्यात चिखलामुळे लोकांना चालणेही कठीण होते.\",\n",
        "    \"शेतकऱ्यांसाठी नवीन अनुदान कधी मिळणार? सरकारी मदतीचा लाभ आम्हाला मिळावा.\",\n",
        "    \"विजेचा सतत पुरवठा खंडित होतो. आम्हाला खूप अडचण होते, कृपया काहीतरी उपाय करा.\",\n",
        "    \"सरकारी रुग्णालयात औषधांचा तुटवडा आहे. गरजू रुग्णांसाठी उपाययोजना करा.\",\n",
        "]\n",
        "\n",
        "# Tokenize test grievances\n",
        "inputs = tokenizer(test_texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512).to(device)\n"
      ],
      "metadata": {
        "id": "wtebBWYRz5hd"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    predictions = torch.argmax(outputs.logits, dim=-1).cpu().numpy()\n",
        "\n",
        "# Define department mapping (should match training labels)\n",
        "department_map = {\n",
        "    0: \"School Education and Sports\",\n",
        "    1: \"Agriculture Department\",\n",
        "    2: \"Co-operation, Marketing and Textiles Department\",\n",
        "    3: \"Dairy Development Department\",\n",
        "    4: \"Department Of Animal Husbandry\",\n",
        "    5: \"Department Of Minority Development\",\n",
        "    6: \"Department of Industries, Energy, Labor and Mines\",\n",
        "    7: \"Department of Sainik Welfare\",\n",
        "    8: \"Environment and Climate Change Department\",\n",
        "    9: \"Finance Department\",\n",
        "    10: \"Fisheries Department\",\n",
        "    11: \"Food, Civil Supplies and Consumer Protection Department\",\n",
        "    12: \"Forest Department\",\n",
        "    13: \"General Administration Department\",\n",
        "    14: \"Goods and Services Tax Department\",\n",
        "    15: \"Greater Mumbai Police\",\n",
        "    16: \"Higher and Technical Education Department\",\n",
        "    17: \"Home Department\",\n",
        "    18: \"Housing Department\",\n",
        "    19: \"Labour Department\",\n",
        "    20: \"Law and Judiciary Department\"\n",
        "}\n",
        "\n",
        "# Print predictions\n",
        "for text, label in zip(test_texts, predictions):\n",
        "    print(f\"Complaint: {text}\\nPredicted Department: {department_map[label]}\\n{'-'*50}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3vllt2p2vFW",
        "outputId": "7ba771ad-121c-4b74-966c-71c077a120cc"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Complaint: माझ्या गावातील रस्त्याची अवस्था खूपच वाईट आहे. पावसाळ्यात चिखलामुळे लोकांना चालणेही कठीण होते.\n",
            "Predicted Department: Department Of Minority Development\n",
            "--------------------------------------------------\n",
            "Complaint: शेतकऱ्यांसाठी नवीन अनुदान कधी मिळणार? सरकारी मदतीचा लाभ आम्हाला मिळावा.\n",
            "Predicted Department: Department Of Animal Husbandry\n",
            "--------------------------------------------------\n",
            "Complaint: विजेचा सतत पुरवठा खंडित होतो. आम्हाला खूप अडचण होते, कृपया काहीतरी उपाय करा.\n",
            "Predicted Department: Department Of Minority Development\n",
            "--------------------------------------------------\n",
            "Complaint: सरकारी रुग्णालयात औषधांचा तुटवडा आहे. गरजू रुग्णांसाठी उपाययोजना करा.\n",
            "Predicted Department: Department Of Animal Husbandry\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r fine_tuned_model.zip fine_tuned_indic_bert"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhI8ZoE738N3",
        "outputId": "0c0d4596-9702-45fb-c4d4-83793c654bfb"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: fine_tuned_indic_bert/ (stored 0%)\n",
            "  adding: fine_tuned_indic_bert/model.safetensors (deflated 7%)\n",
            "  adding: fine_tuned_indic_bert/special_tokens_map.json (deflated 49%)\n",
            "  adding: fine_tuned_indic_bert/spiece.model (deflated 60%)\n",
            "  adding: fine_tuned_indic_bert/tokenizer_config.json (deflated 74%)\n",
            "  adding: fine_tuned_indic_bert/tokenizer.json (deflated 77%)\n",
            "  adding: fine_tuned_indic_bert/config.json (deflated 62%)\n"
          ]
        }
      ]
    }
  ]
}